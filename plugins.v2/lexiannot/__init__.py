import os
import re
import sys
import json
import subprocess
import time
import threading
import queue
import shutil
from typing import Any, List, Dict, Tuple, Optional, Union,  Type, TypeVar
import venv
from pathlib import Path
from collections import Counter

from apscheduler.schedulers.background import BackgroundScheduler
import pysubs2
from pysubs2 import SSAFile, SSAEvent
import pymediainfo
from langdetect import detect

from app.core.config import settings
from app.log import logger
from app.plugins import _PluginBase
from app.core.cache import cached
from app.core.event import eventmanager, Event
from app.utils.system import SystemUtils
from app.schemas.types import NotificationType
from app.utils.http import RequestUtils
from app.utils.string import StringUtils
from app.schemas import TransferInfo
from app.schemas.types import EventType
from app.core.context import MediaInfo
from app.plugins.lexiannot.query_gemini import DialogueTranslationTask, VocabularyTranslationTask, Vocabulary, Context

T = TypeVar('T', VocabularyTranslationTask, DialogueTranslationTask)

class LexiAnnot(_PluginBase):
    # æ’ä»¶åç§°
    plugin_name = "ç¾å‰§ç”Ÿè¯æ ‡æ³¨"
    # æ’ä»¶æè¿°
    plugin_desc = "æ ¹æ®CEFRç­‰çº§ï¼Œä¸ºè‹±è¯­å½±è§†å‰§æ ‡æ³¨é«˜çº§è¯æ±‡ã€‚"
    # æ’ä»¶å›¾æ ‡
    plugin_icon = "https://raw.githubusercontent.com/wumode/LexiAnnot/refs/heads/master/LexiAnnot.png"
    # æ’ä»¶ç‰ˆæœ¬
    plugin_version = "1.0.1"
    # æ’ä»¶ä½œè€…
    plugin_author = "wumode"
    # ä½œè€…ä¸»é¡µ
    author_url = "https://github.com/wumode"
    # æ’ä»¶é…ç½®é¡¹IDå‰ç¼€
    plugin_config_prefix = "lexiannot_"
    # åŠ è½½é¡ºåº
    plugin_order = 50
    # å¯ä½¿ç”¨çš„ç”¨æˆ·çº§åˆ«
    auth_level = 1

    _enabled: bool = False
    _annot_level = ''
    _send_notify = False
    _onlyonce = False
    _show_vocabulary_detail = False
    _show_phonetics = False
    _sentence_translation = False
    _in_place = False
    _enable_gemini = False
    _gemini_model = False
    _gemini_apikey = ''
    _context_window = 0
    _max_retries = 0
    _request_interval = 0
    _ffmpeg_path = ''
    _english_only = False
    _when_file_trans = False
    _model_temperature = ''
    _custom_files = ''
    _accent_color = ''
    _font_scaling = ''
    _opacity = ''

    # æ’ä»¶æ•°æ®
    _lexicon_version = ''
    _swear_words = None
    _cefr_lexicon = None
    _coca2k_lexicon = None

    # protected variables
    _lexicon_repo = 'https://raw.githubusercontent.com/wumode/LexiAnnot/'
    _spacy_model_name = "en_core_web_sm"
    _scheduler: Optional[BackgroundScheduler] = None
    _nlp = None
    _worker_thread = None
    _task_queue = None
    _shutdown_event = None
    _client = None
    _total_token_count = 0
    _venv_python = None
    _query_gemini_script = ''
    _gemini_available = False
    _accent_color_rgb = None
    _color_alpha = 0

    def init_plugin(self, config=None):
        self._task_queue = queue.Queue()
        self.stop_service()
        if config:
            self._enabled = config.get("enabled")
            self._annot_level = config.get("annot_level")
            self._send_notify = config.get("send_notify")
            self._onlyonce = config.get("onlyonce")
            self._show_vocabulary_detail = config.get("show_vocabulary_detail")
            self._sentence_translation = config.get("sentence_translation")
            self._in_place = config.get("in_place")
            self._enable_gemini = config.get("enable_gemini")
            self._gemini_model = config.get("gemini_model")
            self._gemini_apikey = config.get("gemini_apikey")
            self._context_window = config.get("context_window")
            self._max_retries = config.get("max_retries")
            self._request_interval = config.get("request_interval")
            self._ffmpeg_path = config.get("ffmpeg_path")
            self._english_only = config.get("english_only")
            self._when_file_trans = config.get("when_file_trans")
            self._model_temperature = config.get("model_temperature")
            self._show_phonetics = config.get("show_phonetics")
            self._custom_files = config.get("custom_files")
            self._accent_color = config.get("accent_color")
            self._font_scaling = config.get("font_scaling")
            self._opacity = config.get("opacity")

            self._accent_color_rgb = LexiAnnot.hex_to_rgb(self._accent_color) or (255, 255, 0)
            self._color_alpha = int(self._opacity) if self._opacity and len(self._opacity) else 0
        if self._enabled:
            self._query_gemini_script = f"{settings.ROOT_PATH}/app/plugins/lexiannot/query_gemini.py"
            self._cefr_lexicon = self.get_data("cefr_lexicon")
            self._coca2k_lexicon = self.get_data("coca2k_lexicon")
            self._swear_words = self.get_data("swear_words")
            self._lexicon_version = self.get_data("lexicon_version")
            latest = self.__load_lexicon_version()
            if not self._lexicon_version or StringUtils.compare_version(self._lexicon_version, '<', latest):
                self.__load_lexicon()
            # try to import spaCy
            try:
                import spacy
            except ModuleNotFoundError:
                logger.info('æ­£åœ¨å®‰è£…spaCy ...')
                result, output = SystemUtils.execute_with_subprocess(
                    [sys.executable, "-m", "pip", "install", 'thinc==8.3.4']
                )
                if not result:
                    logger.error(f"æ— æ³•å®‰è£…spaCy, {output}")
                    return
                result, output = SystemUtils.execute_with_subprocess(
                    [sys.executable, "-m", "pip", "install", 'spacy==3.8.7']
                )
                if not result:
                    logger.error(f"æ— æ³•å®‰è£…spaCy, {output}")
                    return
            try:
                import spacy
                from spacy.util import compile_infix_regex
                from spacy.tokenizer import Tokenizer
                if self._nlp is None:
                    self._nlp = spacy.load(self._spacy_model_name)
                    infixes = list(self._nlp.Defaults.infixes)
                    infixes = [i for i in infixes if '-' not in i]
                    # ä½¿ç”¨ä¿®æ”¹åçš„æ­£åˆ™è¡¨è¾¾å¼é‡æ–°åˆ›å»º tokenizer
                    infix_re = compile_infix_regex(infixes)
                    self._nlp.tokenizer = Tokenizer(
                        self._nlp.vocab,
                        prefix_search=self._nlp.tokenizer.prefix_search,
                        suffix_search=self._nlp.tokenizer.suffix_search,
                        infix_finditer=infix_re.finditer,
                        token_match=self._nlp.tokenizer.token_match
                    )
            except OSError:
                self._nlp = LexiAnnot.__load_spacy_model(self._spacy_model_name)
            if not (self._nlp and self._cefr_lexicon and self._coca2k_lexicon and self._swear_words):
                _loaded = False
            else:
                _loaded = True
            if not _loaded:
                logger.warn(f"æ’ä»¶æ•°æ®æœªåŠ è½½,åˆå§‹åŒ–å¤±è´¥")
                self._enabled = False
                self.__update_config()
                return
            if self._enable_gemini:
                self._gemini_available = True
                res = self.init_venv()
                if not res:
                    self._gemini_available = False
                if not self._gemini_apikey:
                    logger.warn(f"æœªæä¾›GEMINI APIKEY")
                    self._gemini_available = False
            self._shutdown_event = threading.Event()
            self._worker_thread = threading.Thread(target=self.__process_tasks, daemon=True)
            self._worker_thread.start()
            if self._onlyonce:
                for file_path in self._custom_files.split("\n"):
                    if not file_path:
                        continue
                    self.add_media_file(file_path)
                self._onlyonce = False
                self.__update_config()


    def get_form(self) -> Tuple[List[dict], Dict[str, Any]]:
        """
        æ‹¼è£…æ’ä»¶é…ç½®é¡µé¢ï¼Œéœ€è¦è¿”å›ä¸¤å—æ•°æ®ï¼š1ã€é¡µé¢é…ç½®ï¼›2ã€æ•°æ®ç»“æ„
        """
        return [
            {
                'component': 'VForm',
                'content': [
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'enabled',
                                            'label': 'å¯ç”¨æ’ä»¶',
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'when_file_trans',
                                            'label': 'ç›‘æ§å…¥åº“',
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                        'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'send_notify',
                                            'label': 'å‘é€é€šçŸ¥',
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'onlyonce',
                                            'label': 'ç«‹å³è¿è¡Œä¸€æ¬¡',
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSelect',
                                        'props': {
                                            'model': 'annot_level',
                                            'label': 'æ ‡æ³¨è¯æ±‡çš„æœ€ä½CEFRç­‰çº§',
                                            'items': [
                                                {'title': 'B1', 'value': 'B1'},
                                                {'title': 'B2', 'value': 'B2'},
                                                {'title': 'C1', 'value': 'C1'},
                                                {'title': 'C2', 'value': 'C2'},
                                                {'title': 'C2+', 'value': 'C2+'}
                                            ]
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSelect',
                                        'props': {
                                            'model': 'font_scaling',
                                            'label': 'å­—ä½“ç¼©æ”¾',
                                            'items': [
                                                {'title': '50%', 'value': '0.5'},
                                                {'title': '75%', 'value': '0.75'},
                                                {'title': '100%', 'value': '1'},
                                                {'title': '125%', 'value': '1.25'},
                                                {'title': '150%', 'value': '1.5'},
                                                {'title': '200%', 'value': '2'}
                                            ]
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3,
                                },
                                'content': [
                                    {
                                        'component': 'VTextField',
                                        'props': {
                                            'model': 'accent_color',
                                            'label': 'å¼ºè°ƒè‰²',
                                            'placeholder': '#FFFF00'
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSelect',
                                        'props': {
                                            'model': 'opacity',
                                            'label': 'ä¸é€æ˜åº¦',
                                            'items': [
                                                {'title': '0', 'value': '0'},
                                                {'title': '25%', 'value': '63'},
                                                {'title': '50%', 'value': '127'},
                                                {'title': '75%', 'value': '191'},
                                                {'title': '100%', 'value': '255'},
                                            ]
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'show_phonetics',
                                            'label': 'æ ‡æ³¨éŸ³æ ‡',
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'in_place',
                                            'label': 'åœ¨åŸå­—å¹•æ’å…¥æ³¨é‡Š',
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'english_only',
                                            'label': 'ä»…è‹±è¯­å½±è§†å‰§',
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'show_vocabulary_detail',
                                            'label': 'æ˜¾ç¤ºå®Œæ•´é‡Šä¹‰',
                                        }
                                    }
                                ]
                            },

                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 6,
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'enable_gemini',
                                            'label': 'å¯ç”¨Geminiç¿»è¯‘',
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 6
                                },
                                'content': [
                                    {
                                        'component': 'VSwitch',
                                        'props': {
                                            'model': 'sentence_translation',
                                            'label': 'æ•´å¥ç¿»è¯‘',
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 6,
                                },
                                'content': [
                                    {
                                        'component': 'VSelect',
                                        'props': {
                                            'model': 'gemini_model',
                                            'label': 'æ¨¡å‹',
                                            'items': [
                                                {'title': 'gemini-2.5-flash-preview-05-20',
                                                 'value': 'gemini-2.5-flash-preview-05-20'},
                                                {'title': 'gemini-2.5-pro-preview-05-06',
                                                 'value': 'gemini-2.5-pro-preview-05-06'},
                                                {'title': 'gemini-2.0-flash', 'value': 'gemini-2.0-flash'},
                                                {'title': 'gemini-2.0-flash-lite', 'value': 'gemini-2.0-flash-lite'},
                                                {'title': 'gemini-1.5-flash', 'value': 'gemini-1.5-flash'},
                                                {'title': 'gemini-1.5-pro', 'value': 'gemini-1.5-pro'}
                                            ]
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 6,
                                },
                                'content': [
                                    {
                                        'component': 'VTextField',
                                        'props': {
                                            'model': 'gemini_apikey',
                                            'label': 'Gemini APIKEY',
                                            'placeholder': ''
                                        }
                                    }
                                ]
                            },
                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3,
                                },
                                'content': [
                                    {
                                        'component': 'VTextField',
                                        'props': {
                                            'model': 'context_window',
                                            'label': 'ä¸Šä¸‹æ–‡çª—å£å¤§å°',
                                            'placeholder': '10',
                                            'type': 'number',
                                            'max': 20,
                                            'min': 1,
                                            'hint': 'å‘Geminiå‘é€çš„ä¸Šä¸‹æ–‡é•¿åº¦'
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3
                                },
                                'content': [
                                    {
                                        'component': 'VSelect',
                                        'props': {
                                            'model': 'model_temperature',
                                            'label': 'æ¨¡å‹æ¸©åº¦',
                                            'items': [
                                                {'title': '0', 'value': '0'},
                                                {'title': '0.1', 'value': '0.1'},
                                                {'title': '0.2', 'value': '0.2'},
                                                {'title': '0.3', 'value': '0.3'},
                                            ]
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3,
                                },
                                'content': [
                                    {
                                        'component': 'VTextField',
                                        'props': {
                                            'model': 'max_retries',
                                            'label': 'è¯·æ±‚é‡è¯•æ¬¡æ•°',
                                            'placeholder': '3',
                                            'type': 'number',
                                            'min': 1,
                                            'hint': 'è¯·æ±‚å¤±è´¥é‡è¯•æ¬¡æ•°'
                                        }
                                    }
                                ]
                            },
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                    'md': 3,
                                },
                                'content': [
                                    {
                                        'component': 'VTextField',
                                        'props': {
                                            'model': 'request_interval',
                                            'label': 'è¯·æ±‚é—´éš”',
                                            'type': 'number',
                                            'placeholder': 5    ,
                                            'min': 1,
                                            'suffix': 'ç§’',
                                            'hint': 'è¯·æ±‚é—´éš”æ—¶é—´ï¼Œå»ºè®®ä¸å°‘äº3ç§’'
                                        }
                                    }
                                ]
                            },
                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                },
                                'content': [
                                    {
                                        'component': 'VTextField',
                                        'props': {
                                            'model': 'ffmpeg_path',
                                            'label': 'FFmpeg è·¯å¾„',
                                            'placeholder': 'ffmpeg'
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                },
                                'content': [
                                    {
                                        'component': 'VTextarea',
                                        'props': {
                                            'model': 'custom_files',
                                            'label': 'è§†é¢‘è·¯å¾„',
                                            'rows': 3,
                                            'placeholder': 'æ¯è¡Œä¸€ä¸ªæ–‡ä»¶'
                                        }
                                    }
                                ]
                            },
                        ]
                    },
                    {
                        'component': 'VRow',
                        'content': [
                            {
                                'component': 'VCol',
                                'props': {
                                    'cols': 12,
                                },
                                'content': [
                                    {
                                        'component': 'VAlert',
                                        'props': {
                                            'type': 'success',
                                            'variant': 'tonal'
                                        },
                                        'content': [
                                            {
                                                'component': 'span',
                                                'text': 'é…ç½®è¯´æ˜ï¼š'
                                            },
                                            {
                                                'component': 'a',
                                                'props': {
                                                    'href': 'https://github.com/jxxghp/MoviePilot-Plugins/tree/main/plugins.v2/lexiannot/README.md',
                                                    'target': '_blank'
                                                },
                                                'content': [
                                                    {
                                                        'component': 'u',
                                                        'text': 'README'
                                                    }
                                                ]
                                            }]
                                    }

                                ]
                            }
                        ]
                    }
                ]
            }
        ], {
            "enabled": False,
            "annot_level": 'C1',
            "send_notify": False,
            "onlyonce": False,
            "show_vocabulary_detail": False,
            "show_phonetics": False,
            "sentence_translation": False,
            "in_place": False,
            "enable_gemini": False,
            "gemini_model": 'gemini-2.0-flash',
            "gemini_apikey": '',
            "context_window": 10,
            "max_retries": 3,
            'request_interval': 3,
            "ffmpeg_path": "",
            "english_only": True,
            "when_file_trans": True,
            "model_temperature": '0.3',
            "custom_files": '',
            "accent_color": '',
            "font_scaling": '1',
            "opacity": '0',
        }

    def get_api(self) -> List[Dict[str, Any]]:
        pass

    def get_page(self) -> List[dict]:
        pass

    @staticmethod
    def get_command() -> List[Dict[str, Any]]:
        pass

    def get_state(self) -> bool:
        """
        è·å–æ’ä»¶çŠ¶æ€ï¼Œå¦‚æœæ’ä»¶æ­£åœ¨è¿è¡Œï¼Œ åˆ™è¿”å›True
        """
        return self._enabled

    def stop_service(self):
        """
        é€€å‡ºæ’ä»¶
        """
        self.shutdown()

    def shutdown(self):
        """
        å…³é—­æ’ä»¶
        """
        if self._worker_thread and self._worker_thread.is_alive():
            logger.debug("ğŸ”» Stopping existing worker thread...")
            self._shutdown_event.set()
            self._worker_thread.join()
            logger.debug("âœ… Existing worker thread stopped.")
        else:
            logger.debug("â„¹ï¸ No running worker thread to stop.")

    def add_media_file(self, path: str):
        """
        æ·»åŠ æ–°ä»»åŠ¡
        """
        if not self._shutdown_event.is_set():
            self._task_queue.put(path)
        else:
            raise RuntimeError("Plugin is shutting down. Cannot add new tasks.")

    def __update_config(self):
        self.update_config({'enabled': self._enabled,
                            'annot_level': self._annot_level,
                            'send_notify': self._send_notify,
                            'onlyonce': self._onlyonce,
                            'show_vocabulary_detail': self._show_vocabulary_detail,
                            'sentence_translation': self._sentence_translation,
                            'in_place': self._in_place,
                            'enable_gemini': self._enable_gemini,
                            'gemini_model': self._gemini_model,
                            'gemini_apikey': self._gemini_apikey,
                            'context_window': self._context_window,
                            'max_retries': self._max_retries,
                            'request_interval': self._request_interval,
                            'ffmpeg_path': self._ffmpeg_path,
                            'english_only': self._english_only,
                            'when_file_trans': self._when_file_trans,
                            'model_temperature': self._model_temperature,
                            'show_phonetics': self._show_phonetics,
                            'custom_files': self._custom_files,
                            'accent_color': self._accent_color,
                            'font_scaling': self._font_scaling,
                            'opacity': self._opacity
                            })

    def __process_tasks(self):
        """
        åå°çº¿ç¨‹ï¼šå¤„ç†ä»»åŠ¡é˜Ÿåˆ—
        """
        logger.debug("ğŸ‘· Worker thread started.")
        while not self._shutdown_event.is_set():
            try:
                task = self._task_queue.get(timeout=1)  # æœ€å¤šç­‰å¾…1ç§’
                if task is None:
                    continue
                self.__process_file(task)
            except queue.Empty:
                continue
        logger.debug("ğŸ›‘ Worker received shutdown signal, exiting...")

    def __process_file(self, path: str):
        """
        å¤„ç†è§†é¢‘æ–‡ä»¶
        """
        video = Path(path)
        if video.suffix.lower() not in settings.RMT_MEDIAEXT:
            return
        if not video.exists() or not video.is_file():
            logger.warn(f"æ–‡ä»¶ {str(video)} ä¸å­˜åœ¨, è·³è¿‡")
            return
        subtitle = video.with_suffix(".ass")
        if subtitle.exists():
            logger.warn(f"å­—å¹•æ–‡ä»¶ ({subtitle}) å·²å­˜åœ¨, è·³è¿‡")
            return
        logger.info(f"ğŸ“‚ Processing file: {path}")
        if self._send_notify:
            message = f"æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š {path}"
            self.post_message(title=f"ã€{self.plugin_name}ã€‘",
                              mtype=NotificationType.Plugin,
                              text=f"{message}")
        ffmpeg_path = self._ffmpeg_path if self._ffmpeg_path else 'ffmpeg'
        embedded_subtitles = LexiAnnot.__extract_subtitles_by_lang(path, 'en', ffmpeg_path)
        ret_message = ''
        if embedded_subtitles:
            logger.info(f'æå–åˆ° {len(embedded_subtitles)} æ¡è‹±è¯­æ–‡æœ¬å­—å¹•')
            for embedded_subtitle in embedded_subtitles:
                if self._shutdown_event.is_set():
                    return
                ass_subtitle = pysubs2.SSAFile.from_string(embedded_subtitle['subtitle'], format_='ass')
                if embedded_subtitle.get('codec_id') == 'S_TEXT/UTF8':
                    ass_subtitle = LexiAnnot.set_srt_style(ass_subtitle)
                ass_subtitle = self.__set_style(ass_subtitle)
                ass_subtitle = self.process_subtitles(ass_subtitle)
                if self._shutdown_event.is_set():
                    return
                if ass_subtitle:
                    try:
                        ass_subtitle.save(str(subtitle))
                        ret_message = f"å­—å¹•å·²ä¿å­˜ï¼š{str(subtitle)}"
                    except Exception as e:
                        logger.error(f"å­—å¹•æ–‡ä»¶ {subtitle} ä¿å­˜å¤±è´¥, {e}")
                    break
                else:
                    logger.info(f"å¤„ç†å­—å¹•{embedded_subtitle['codec_id']}-{embedded_subtitle['stream_id']}å¤±è´¥")
        else:
            logger.warn(f"æœªèƒ½åœ¨{path}ä¸­æ‰¾åˆ°å¯æå–çš„è‹±æ–‡å­—å¹•")
        if not ret_message:
            ret_message= f"æœªèƒ½åœ¨{path}ä¸­æ‰¾åˆ°å¯æå–çš„è‹±æ–‡å­—å¹•"
        logger.info(f"âœ… Finished: {path}")
        if self._send_notify:
            self.post_message(title=f"ã€{self.plugin_name}ã€‘",
                              mtype=NotificationType.Plugin,
                              text=f"{ret_message}")

    @cached(maxsize=1000, ttl=1800)
    def __load_lexicon_version(self) -> Optional[str]:
        logger.info(f"æ­£åœ¨æ£€æŸ¥è¿œç¨‹è¯åº“ç‰ˆæœ¬...")
        url = f'{self._lexicon_repo}master/version'
        version = RequestUtils().get(url, headers=settings.REPO_GITHUB_HEADERS())
        if version is None:
            return None
        return version

    def __load_lexicon(self):
        url = f'{self._lexicon_repo}master/cefr.json'
        res = RequestUtils().get_res(url, headers=settings.REPO_GITHUB_HEADERS())
        if res:
            self._cefr_lexicon = res.json()
        url = f'{self._lexicon_repo}master/coca20k.json'
        res = RequestUtils().get_res(url, headers=settings.REPO_GITHUB_HEADERS())
        if res:
            self._coca2k_lexicon = res.json()
        url = f'{self._lexicon_repo}master/swear_words.json'
        res = RequestUtils().get_res(url, headers=settings.REPO_GITHUB_HEADERS())
        if res:
            self._swear_words = res.json()
        self._lexicon_version = self.__load_lexicon_version()
        self.save_data("cefr_lexicon", self._cefr_lexicon)
        self.save_data("coca2k_lexicon", self._coca2k_lexicon)
        self.save_data("swear_words", self._swear_words)
        self.save_data("lexicon_version", self._lexicon_version)

    @staticmethod
    def __load_spacy_model(model_name: str):
        try:
            import spacy
            result = subprocess.run(
                [sys.executable, "-m", "spacy", "download", model_name],
                capture_output=True,
                text=True,
                check=True
            )
            nlp = spacy.load(model_name)
            logger.info(f"spaCy æ¨¡å‹ '{model_name}' åŠ è½½æˆåŠŸï¼")
            return nlp
        except subprocess.CalledProcessError as e:
            logger.error(f"ä¸‹è½½ spaCy æ¨¡å‹ '{model_name}' å¤±è´¥ã€‚")
            logger.error(f"å‘½ä»¤è¿”å›éé›¶é€€å‡ºç ï¼š{e.returncode}")
            logger.error(f"Stdout:\n{e.stdout}")
            logger.error(f"Stderr:\n{e.stderr}")
            return None
        except Exception as e:
            logger.error(f"ä¸‹è½½æˆ–åŠ è½½ spaCy æ¨¡å‹æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")
            return None


    @eventmanager.register(EventType.TransferComplete)
    def check_media(self, event: Event):
        if not self._enabled or not self._when_file_trans:
            return
        event_info: dict = event.event_data
        if not event_info:
            return

        # å…¥åº“æ•°æ®
        transfer_info: TransferInfo = event_info.get("transferinfo")
        if not transfer_info or not transfer_info.target_diritem or not transfer_info.target_diritem.path:
            return
        mediainfo: MediaInfo = event_info.get("mediainfo")
        if self._english_only:
            if mediainfo.original_language != 'en':
                logger.info(f"åŸå§‹è¯­è¨€ ({mediainfo.original_language}) ä¸ä¸ºè‹±è¯­, è·³è¿‡ {mediainfo.title}ï¼š ")
                return
        for new_path in transfer_info.file_list_new:
            self.add_media_file(new_path)

    @staticmethod
    def query_cefr(word, cefr_lexicon):
        word = word.lower().strip("-*'")
        if word in cefr_lexicon:
            return cefr_lexicon[word]
        else:
            return None

    @staticmethod
    def query_coca20k(word: str, lexicon: Dict[str, Any]):
        word = word.lower().strip("-*'")
        return lexicon.get(word)

    @staticmethod
    def convert_pos_to_spacy(pos: str):
        """
        å°†ç»™å®šçš„è¯æ€§åˆ—è¡¨è½¬æ¢ä¸ºspaCyåº“ä¸­ä½¿ç”¨çš„è¯æ€§æ ‡ç­¾ã€‚

        Args:
          pos: ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼è¯æ€§ã€‚

        Returns:
          ä¸€ä¸ªåŒ…å«å¯¹åº”spaCyè¯æ€§æ ‡ç­¾çš„åˆ—è¡¨ã€‚å¯¹äºæ— æ³•ç›´æ¥æ˜ å°„çš„è¯æ€§ï¼Œ
          å°†è¿”å›Noneã€‚
        """
        spacy_pos_map = {
            'noun': 'NOUN',
            'adjective': 'ADJ',
            'adverb': 'ADV',
            'verb': 'VERB',
            'preposition': 'ADP',
            'conjunction': 'CCONJ',
            'determiner': 'DET',
            'pronoun': 'PRON',
            'interjection': 'INTJ',
            'number': 'NUM'
        }

        pos_lower = pos.lower()
        if pos_lower in spacy_pos_map:
            spacy_pos = spacy_pos_map[pos_lower]
        elif pos_lower == 'be-verb':
            spacy_pos = 'AUX'  # Auxiliary verb (e.g., be, do, have)
        elif pos_lower == 'vern':
            spacy_pos = 'VERB'  # Assuming 'vern' is a typo for 'verb'
        elif pos_lower == 'modal auxiliary':
            spacy_pos = 'AUX'  # Modal verbs are also auxiliaries
        elif pos_lower == 'do-verb':
            spacy_pos = 'AUX'
        elif pos_lower == 'have-verb':
            spacy_pos = 'AUX'
        elif pos_lower == 'infinitive-to':
            spacy_pos = 'PART'  # Particle (e.g., to in "to go")
        elif not pos_lower:  # Handle empty strings
            spacy_pos = None
        else:
            spacy_pos = None  # For unmapped POS tags
        return spacy_pos

    @staticmethod
    def get_cefr_by_spacy(lemma_: str, pos_: str, cefr_lexicon: Dict[str, Any]) -> Optional[str]:
        result = LexiAnnot.query_cefr(lemma_, cefr_lexicon)
        if result:
            all_cefr = []
            if len(result) > 0:
                for entry in result:
                    if pos_ == LexiAnnot.convert_pos_to_spacy(entry['pos']):
                        return entry['cefr']
                    all_cefr.append(entry['cefr'])
            return min(all_cefr)
        return None

    @staticmethod
    def format_duration(ms):
        total_seconds, milliseconds = divmod(ms, 1000)
        hours, remainder = divmod(total_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        hundredths = milliseconds // 10
        return f"{hours}:{minutes:02}:{seconds:02}.{hundredths:02}"

    @staticmethod
    def replace_by_plaintext_positions(line: SSAEvent, replacements: List[dict]):
        """
        æ›¿æ¢ line.text ä¸­çš„å†…å®¹ï¼Œä½¿ç”¨ replacements ä¸­çš„ plaintext ä½ç½®ä¿¡æ¯ã€‚
        replacement
        {'start': int, 'end': int, 'old_text': str, 'new_text': str}
        """
        text = line.text
        tag_pattern = re.compile(r"{.*?}")  # åŒ¹é… {xxx} æ ¼å¼æ§åˆ¶ç¬¦
        special_pattern = re.compile(r"\\[Nh]")
        # æ„å»º plaintext ä½ç½®åˆ° text ç´¢å¼•çš„æ˜ å°„
        mapping = {}  # plaintext_index -> text_index
        p_index = 0  # å½“å‰ plaintext ç´¢å¼•
        t_index = 0  # å½“å‰ text ç´¢å¼•

        while t_index < len(text):
            if text[t_index] == "{":
                # è·³è¿‡æ ¼å¼æ ‡ç­¾
                match = tag_pattern.match(text, t_index)
                if match:
                    t_index = match.end()
                    continue
            elif text[t_index] == "\\":
                match = special_pattern.match(text, t_index)
                if match:
                    t_index = match.end() - 1
                    continue
            # éæ ¼å¼å­—ç¬¦
            mapping[p_index] = t_index
            p_index += 1
            t_index += 1

        # æŒ‰ç…§ mapping æ‰§è¡Œæ›¿æ¢ï¼ˆå€’åºæ›¿æ¢é˜²æ­¢ä½ç½®é”™ä½ï¼‰
        new_text = text
        for r in sorted(replacements, key=lambda x: x["start"], reverse=True):
            start = mapping.get(r["start"])
            end = mapping.get(r["end"] - 1)
            if start is None or end is None:
                continue
            end += 1  # å› ä¸º Python åˆ‡ç‰‡ä¸åŒ…å«ç»“æŸç´¢å¼•
            new_text = new_text[:start] + r["new_text"] + new_text[end:]

        line.text = new_text

    @staticmethod
    def analyze_ass_language(ass_file: SSAFile):
        styles = {}
        for style in ass_file.styles:
            styles[style] = {'text': [], 'duration': 0, 'text_size': 0, 'times': 0}
        for dialogue in ass_file:
            style = dialogue.style
            text = dialogue.plaintext
            sub_text = text.split('\n')
            if style not in styles or not text: continue
            styles[style]['text'].extend(sub_text)
            styles[style]['duration'] += dialogue.duration
            styles[style]['text_size'] += len(text)
            styles[style]['times'] += 1
        style_language_analysis = {}
        for style_name, data in styles.items():
            all_text = ' '.join(data['text'])
            if not all_text.strip():
                style_language_analysis[style_name] = None
                continue

            languages = []
            # å¯¹æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µè¿›è¡Œè¯­è¨€æ£€æµ‹
            for text_fragment in data['text']:
                try:
                    lang = detect(text_fragment)
                    languages.append(lang)
                except:
                    pass  # æ— æ³•æ£€æµ‹çš„æ–‡æœ¬

            if languages:
                language_counts = Counter(languages)
                most_common_language = language_counts.most_common(1)[0]
                style_language_analysis[style_name] = {"main_language": most_common_language[0],
                                                       "proportion": most_common_language[1] / len(languages),
                                                       "duration": data['duration'],
                                                       "text_size": data['text_size'],
                                                       "times": data['times']}
            else:
                style_language_analysis[style_name] = None

        return style_language_analysis

    @staticmethod
    def select_main_style_weighted(language_analysis: Dict[str, Any], known_language: str,
                                   weights=None):
        """
        æ ¹æ®è¯­è¨€åˆ†æç»“æœå’Œå·²çŸ¥çš„å­—å¹•è¯­è¨€ï¼Œä½¿ç”¨åŠ æƒè¯„åˆ†é€‰æ‹©ä¸»è¦æ ·å¼ã€‚

        Args:
            language_analysis (dict): `analyze_ass_language` å‡½æ•°çš„è¾“å‡ºç»“æœã€‚
            known_language (str): å·²çŸ¥çš„å­—å¹•è¯­è¨€ä»£ç .
            weights (dict): å„ä¸ªç»´åº¦çš„æƒé‡ï¼Œæƒé‡ä¹‹å’Œåº”ä¸º 1.

        Returns:
            str or None: ä¸»è¦å­—å¹•çš„æ ·å¼åç§°ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…çš„æ ·å¼åˆ™è¿”å› Noneã€‚
        """
        if weights is None:
            weights = {'times': 0.5, 'text_size': 0.4, 'duration': 0.1}
        matching_styles = []
        max_times = max([analysis.get('times', 0) for _, analysis in language_analysis.items() if analysis]) or 1
        max_text_size = max(
            [analysis.get('text_size', 0) for _, analysis in language_analysis.items() if analysis]) or 1
        max_duration = max([analysis.get('duration', 0) for _, analysis in language_analysis.items() if analysis]) or 1
        for style, analysis in language_analysis.items():
            if not analysis:
                continue
            if analysis.get('main_language') == known_language:
                # è·³è¿‡å¤šè¯­è¨€
                if analysis.get('proportion', 0) < 0.5:
                    continue
                score = 0
                score += analysis.get('times', 0) * weights.get('times', 0) / max_times
                score += analysis.get('text_size', 0) * weights.get('text_size', 0) / max_text_size
                score += analysis.get('duration', 0) * weights.get('duration', 0) / max_duration
                matching_styles.append((style, score))

        if not matching_styles:
            return None

        sorted_styles = sorted(matching_styles, key=lambda item: item[1], reverse=True)
        return sorted_styles[0][0]

    @staticmethod
    def set_srt_style(ass: SSAFile) -> SSAFile:
        ass.info['ScaledBorderAndShadow'] = 'no'
        play_res_y = int(ass.info.get('PlayResY'))
        play_res_x = int(ass.info.get('PlayResX'))
        if 'Default' in ass.styles:
            ass.styles['Default'].marginv = play_res_y // 16
            ass.styles['Default'].fontname = 'Microsoft YaHei'
            ass.styles['Default'].fontsize = play_res_y // 16
        return ass

    def __set_style(self, ass: SSAFile) -> SSAFile:
        font_scaling = float(self._font_scaling) if self._font_scaling and len(self._font_scaling) else 1
        play_res_y = int(ass.info.get('PlayResY'))
        play_res_x = int(ass.info.get('PlayResX'))
        # åˆ›å»ºä¸€ä¸ªæ–°æ ·å¼
        fs = play_res_y // 16*font_scaling
        new_style = pysubs2.SSAStyle()
        new_style.name = 'Annotation EN'
        new_style.fontname = 'Times New Roman'
        new_style.fontsize = fs
        new_style.primarycolor = pysubs2.Color(self._accent_color_rgb[0],
                                               self._accent_color_rgb[1],
                                               self._accent_color_rgb[2],
                                               self._color_alpha)  # é»„è‰² (BGR, alpha)
        new_style.bold = True
        new_style.italic = False
        new_style.outline = 1
        new_style.shadow = 0
        new_style.alignment = pysubs2.Alignment.TOP_LEFT
        new_style.marginl = play_res_x // 20
        new_style.marginr = play_res_x // 20
        new_style.marginv = fs
        ass.styles['Annotation EN'] = new_style
        zh_style = new_style.copy()
        zh_style.name = 'Annotation ZH'
        zh_style.fontname = 'Microsoft YaHei'
        zh_style.primarycolor = pysubs2.Color(255, 255, 255, self._color_alpha)
        ass.styles['Annotation ZH'] = zh_style

        pos_style = zh_style.copy()
        pos_style.name = 'Annotation POS'
        pos_style.fontname = 'Times New Roman'
        pos_style.fontsize = fs * 0.75
        pos_style.italic = True
        ass.styles['Annotation POS'] = pos_style

        phone_style = pos_style.copy()
        phone_style.name = 'Annotation PHONE'
        phone_style.fontname = 'Arial'
        phone_style.fontsize = fs * 0.75
        phone_style.bold = False
        phone_style.italic = False
        ass.styles['Annotation PHONE'] = phone_style

        pos_def_cn_style = zh_style.copy()
        pos_def_cn_style.name = 'DETAIL CN'
        pos_def_cn_style.fontsize = fs * 0.7
        ass.styles['DETAIL CN'] = pos_def_cn_style

        pos_def_pos_style = pos_style.copy()
        pos_def_pos_style.name = 'DETAIL POS'
        pos_def_pos_style.fontsize = fs * 0.6
        ass.styles['DETAIL POS'] = pos_def_pos_style

        cefr_style = pos_style.copy()
        cefr_style.name = "Annotation CEFR"
        cefr_style.fontname = "Times New Roman"
        cefr_style.fontsize = fs * 0.5
        cefr_style.bold = True
        cefr_style.italic = False
        cefr_style.primarycolor = pysubs2.Color(self._accent_color_rgb[0],
                                               self._accent_color_rgb[1],
                                               self._accent_color_rgb[2],
                                               self._color_alpha)
        cefr_style.outline = 1
        cefr_style.shadow = 0
        ass.styles['Annotation CEFR'] = cefr_style
        return ass

    @staticmethod
    def hex_to_rgb(hex_color) -> Optional[Tuple]:
        if not hex_color:
            return None
        pattern = r'^#[0-9a-fA-F]{6}$'
        if re.match(pattern, hex_color) is None:
            return None
        hex_color = hex_color.lstrip('#')  # å»æ‰å‰é¢çš„ #
        return tuple(int(hex_color[i:i + 2], 16) for i in (0, 2, 4))

    @staticmethod
    def __extract_subtitle(video_path: str,
                           subtitle_stream_index: str,
                           ffmpeg_path: str='ffmpeg',
                           sub_format='ass') -> Optional[str]:
        if sub_format not in ['srt', 'ass']:
            raise ValueError('Invalid subtitle format')
        try:
            map_parameter = f"0:s:{subtitle_stream_index}"
            command = [
                ffmpeg_path,
                '-i', video_path,
                '-map', map_parameter,
                '-f', sub_format,
                '-'
            ]
            result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8', check=True)
            return result.stdout
        except FileNotFoundError:
            logger.warn(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶ '{video_path}'")
            return None
        except subprocess.CalledProcessError as e:
            logger.warn(f"é”™è¯¯ï¼šæå–å­—å¹•å¤±è´¥ã€‚\né”™è¯¯ä¿¡æ¯ï¼š{e}")
            logger.warn(f"FFmpeg è¾“å‡º (stderr):\n{e.stderr.decode('utf-8', errors='ignore')}")
            return None

    @staticmethod
    def __extract_subtitles_by_lang(video_path: str, lang: str = 'en', ffmpeg: str = 'ffmpeg') -> Optional[List[Dict]]:
        """æå–è§†é¢‘æ–‡ä»¶ä¸­çš„å†…åµŒè‹±æ–‡å­—å¹•ï¼Œä½¿ç”¨ MediaInfo æŸ¥æ‰¾å­—å¹•æµã€‚"""
        supported_codec = ['S_TEXT/UTF8', 'S_TEXT/ASS']
        subtitles = []
        try:
            media_info: pymediainfo.MediaInfo = pymediainfo.MediaInfo.parse(video_path)
            for track in media_info.tracks:
                if track.track_type == 'Text' and track.language == lang and track.codec_id in supported_codec:
                    if track.title and 'SDH' in track.title:
                        continue
                    subtitle_stream_index = track.stream_identifier  # MediaInfo çš„ stream_id ä» 1 å¼€å§‹ï¼Œffmpeg ä» 0 å¼€å§‹
                    subtitle = LexiAnnot.__extract_subtitle(video_path, subtitle_stream_index, ffmpeg)
                    if subtitle:
                        subtitles.append({'title': track.title, 'subtitle': subtitle, 'codec_id': track.codec_id,
                                          'stream_id': subtitle_stream_index})
            if subtitles:
                return subtitles
            else:
                logger.warn('æœªæ‰¾åˆ°æ ‡è®°ä¸ºè‹±è¯­çš„æ–‡æœ¬å­—å¹•æµ')
                return None

        except FileNotFoundError:
            logger.error(f"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶ '{video_path}'")
            return None
        except subprocess.CalledProcessError as e:
            logger.error(f"é”™è¯¯ï¼šæå–å­—å¹•å¤±è´¥ã€‚\né”™è¯¯ä¿¡æ¯ï¼š{e}")
            logger.error(f"FFmpeg è¾“å‡º (stderr):\n{e.stderr.decode('utf-8', errors='ignore')}")
            return None
        except Exception as e:
            logger.error(f"ä½¿ç”¨ MediaInfo æå–å­—å¹•æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            return None

    def init_venv(self) -> bool:
        venv_dir = os.path.join(self.get_data_path(), "venv_genai")
        python_path = os.path.join(venv_dir, "bin", "python") if os.name != "nt" else os.path.join(venv_dir, "Scripts",
                                                                                                   "python.exe")
        # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
        try:
            if not os.path.exists(venv_dir):
                logger.info(f"ä¸º google-genai åˆå§‹åŒ–è™šæ‹Ÿç¯å¢ƒ: {venv_dir}")
                venv.create(venv_dir, with_pip=True, symlinks=True, clear=True)
                logger.info(f"è™šæ‹Ÿç¯å¢ƒåˆ›å»ºæˆåŠŸ: {venv_dir}")
            SystemUtils.execute_with_subprocess([python_path, "-m", "pip", "install", 'google-genai'])
        except subprocess.CalledProcessError:
            logger.warn(f"è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå¤±è´¥")
            shutil.rmtree(venv_dir)
            return False
        self._venv_python = python_path

        return True

    def __query_gemini(
            self,
            tasks: List[T],
            task_type: Type[T],
            api_key: str,
            system_instruction: str,
            model: str,
            temperature: float
    ) -> List[T]:
        input_dict = {
            'tasks': [task.dict() for task in tasks],  # ä¿è¯æ˜¯å¯åºåˆ—åŒ–æ ¼å¼
            'params': {
                'api_key': api_key,
                'system_instruction': system_instruction,
                'schema': task_type.__name__,
                'model': model,
                'temperature': temperature,
                'max_retries': self._max_retries
            }
        }

        try:
            result = subprocess.run(
                [self._venv_python, self._query_gemini_script],
                input=json.dumps(input_dict),
                capture_output=True,
                text=True,
                check=True
            )
        except subprocess.CalledProcessError as e:
            logger.warning(f"Subprocess failed: {str(e)}")
            return tasks

        try:
            response = json.loads(result.stdout)
        except json.JSONDecodeError:
            logger.warning(f"Invalid JSON from subprocess:\n{result.stdout}")
            return tasks

        if not response.get("success"):
            logger.warning(f"Error in subprocess response: {response.get('message')}")
            return tasks

        try:
            return [task_type(**task_data) for task_data in response["data"]["tasks"]]
        except Exception as e:
            logger.warning(f"Failed to reconstruct tasks: {str(e)}")
            return tasks

    def __process_by_ai(self, lines_to_process: List[Dict[str, Any]], cefr_lexicon, swear_words, coca20k_lexicon):
        simple_vocabulary = list(filter(lambda x:x<self._annot_level, ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))
        patterns = [r'\d+th|\d?1st|\d?2nd']
        compiled_patterns = [re.compile(p) for p in patterns]
        model_temperature = float(self._model_temperature) if self._model_temperature else 0.3
        logger.info(f"é€šè¿‡spaCyåˆ†è¯...")
        vocabulary_trans_instruction = '''You are an expert translator. You will be given a list of English words along with their context, formatted as JSON. For each entry, provide the most appropriate translation in Simplified Chinese based on the context.
    Only complete the `Chinese` field. Do not include pinyin, explanations, or any additional information.'''
        # ä½¿ç”¨nlpåˆ†è¯
        for line_data in lines_to_process:
            if self._shutdown_event.is_set():
                return lines_to_process
            text_raw = line_data.get('raw_subtitle')
            text = text_raw.replace('\n', ' ')
            new_vocab = []
            doc = self._nlp(text)
            last_end_pos = 0
            lemma_to_query = []
            for token in doc:
                if len(token.text) == 1:
                    continue
                if token.lemma_ in swear_words:
                    continue
                if token.pos_ not in ('NOUN', 'AUX', 'VERB', 'ADJ', 'ADV', 'ADP', 'CCONJ', 'SCONJ'):
                    continue
                if any(p.match(token.lemma_) for p in compiled_patterns):
                    continue
                cefr = LexiAnnot.get_cefr_by_spacy(token.lemma_, token.pos_, cefr_lexicon)
                if cefr and cefr in simple_vocabulary:
                    continue
                res_of_coco = LexiAnnot.query_coca20k(token.lemma_, coca20k_lexicon)
                if res_of_coco and not cefr:
                    cefr = 'COCA20K'
                if token.lemma_ in lemma_to_query:
                    continue
                else:
                    lemma_to_query.append(token.lemma_)
                start_pos = text.find(token.text, last_end_pos)
                end_pos = start_pos + len(token.text)
                phonetics = ''
                pos_defs = []
                if res_of_coco:
                    phonetics = res_of_coco.get('phonetics_1') or ''
                    pos_defs = res_of_coco.get('pos_defs') or []
                last_end_pos = end_pos
                new_vocab.append({'start': start_pos, 'end': end_pos, 'text': token.text, 'lemma': token.lemma_,
                                  'pos': token.pos_, 'cefr': cefr, 'Chinese': '', 'phonetics': phonetics,
                                  'pos_defs': pos_defs})
            line_data['new_vocab'] = new_vocab
        # æŸ¥è¯¢è¯æ±‡ç¿»è¯‘
        task_bulk: List[Union[VocabularyTranslationTask|DialogueTranslationTask]] = []
        i = 0
        if self._gemini_available:
            logger.info(f"æŸ¥è¯¢è¯æ±‡ç¿»è¯‘...")
        for line_data in lines_to_process:
            if self._shutdown_event.is_set():
                return lines_to_process
            if not self._gemini_available:
                break
            i += 1
            if not (len(line_data["new_vocab"]) or (i == len(lines_to_process) and len(task_bulk))):
                continue
            new_vocab = [Vocabulary(lemma=new_vocab['lemma'], Chinese='') for new_vocab in line_data['new_vocab']]
            task_bulk.append(VocabularyTranslationTask(index=line_data['index'],
                                                   vocabulary=new_vocab,
                                                   context=Context(
                                                       original_text=line_data['raw_subtitle'].replace('\n', ' ')
                                                   )))
            if len(task_bulk) >= self._context_window or (len(task_bulk) and i == len(lines_to_process)):
                logger.info(f"processing dialogues: "
                      f"{LexiAnnot.format_duration(lines_to_process[task_bulk[0].index]['time_code'][0])} -> "
                      f"{LexiAnnot.format_duration(lines_to_process[i - 1]['time_code'][1])}")
                answer: Optional[List[VocabularyTranslationTask]] = self.__query_gemini(task_bulk,
                                                                                        VocabularyTranslationTask,
                                                                                        self._gemini_apikey,
                                                                                        vocabulary_trans_instruction,
                                                                                        self._gemini_model,
                                                                                        model_temperature)
                if not answer:
                    continue
                time.sleep(self._request_interval)
                for answer_line in answer:
                    answer_lemma = tuple(v.lemma for v in answer_line.vocabulary)
                    filtered_raw = [x for x in lines_to_process if x.get('index') == answer_line.index]
                    if not len(filtered_raw):
                        logger.warn(f'Unknown answer: {answer_line.index}: {answer_line.context.original_text}')
                    available_answer = False
                    for item in filtered_raw:
                        lemma = tuple(v['lemma'] for v in item['new_vocab'])
                        if lemma == answer_lemma:
                            available_answer = True
                            for i_, v in enumerate(item['new_vocab']):
                                v['Chinese'] = answer_line.vocabulary[i_].Chinese
                            break
                    if not available_answer:
                        logger.warn(f'Unknown answer: {answer_line.index}: {answer_line.context.original_text}')
                task_bulk = []
        if not self._sentence_translation:
            return lines_to_process
        if self._gemini_available:
            logger.info(f"æŸ¥è¯¢æ•´å¥ç¿»è¯‘...")
        # æŸ¥è¯¢æ•´å¥ç¿»è¯‘
        translation_tasks: List[DialogueTranslationTask] = []
        for line_data in lines_to_process:
            translation_tasks.append(DialogueTranslationTask(index=line_data['index'],
                                                             original_text=line_data['raw_subtitle'].replace('\n', ' '),
                                                             Chinese=''))
        i = 0
        dialog_trans_instruction = '''You are an expert translator. You will be given a list of dialogue translation tasks in JSON format. For each entry, provide the most appropriate translation in Simplified Chinese based on the context. 
    Only complete the `Chinese` field. Do not include pinyin, explanations, or any additional information.'''
        while i < len(translation_tasks):
            if self._shutdown_event.is_set():
                return lines_to_process
            if not self._gemini_available:
                break
            start_index = max(0, i - 1)
            end_index = min(len(translation_tasks), i + self._context_window + 1)
            task_bulk: List[DialogueTranslationTask] = translation_tasks[start_index:end_index]
            logger.info(f"processing dialogues: "
            f"{LexiAnnot.format_duration(lines_to_process[i]['time_code'][0])} -> "
            f"{LexiAnnot.format_duration(lines_to_process[min(len(translation_tasks), i + self._context_window)-1]['time_code'][1])}")
            answer: List[DialogueTranslationTask] = self.__query_gemini(task_bulk,
                                                                        DialogueTranslationTask,
                                                                        self._gemini_apikey,
                                                                        dialog_trans_instruction,
                                                                        self._gemini_model,
                                                                        model_temperature)
            time.sleep(self._request_interval)
            for answer_line in answer:
                if  answer_line.index not in range(i, i+self._context_window):
                    continue
                filtered_raw = [x for x in lines_to_process if x.get('index') == answer_line.index]
                if not len(filtered_raw):
                    logger.warn(f'Unknown answer: {answer_line.index}: {answer_line.original_text}')
                available_answer = False
                for item in filtered_raw:
                    if item['raw_subtitle'].replace('\n', ' ') == answer_line.original_text:
                        available_answer = True
                        item['Chinese'] = answer_line.Chinese
                        break
                if not available_answer:
                    logger.warn(f'Unknown answer: {answer_line.index}: {answer_line.original_text}')
            i += self._context_window
        return lines_to_process

    def process_subtitles(self, ass_file: SSAFile) -> Optional[SSAFile]:
        """
        å¤„ç†å­—å¹•å†…å®¹ï¼Œæ ‡è®°è¯æ±‡å¹¶æ·»åŠ ç¿»è¯‘ã€‚
        """
        lang = 'en'
        cefr_lexicon = self._cefr_lexicon
        swear_words = self._swear_words
        coca20k_lexicon = self._coca2k_lexicon
        abgr_str = (f'&H{self._color_alpha:02x}{self._accent_color_rgb[2]:02x}'
                                 f'{self._accent_color_rgb[1]:02x}{self._accent_color_rgb[0]:02x}&') #&H00FFFFFF&
        pos_map = {
            'NOUN': 'n.',
            'AUX': 'aux.',
            'VERB': 'v.',
            'ADJ': 'adj.',
            'ADV': 'adv.',
            'ADP': 'prep.',
            'CCONJ': 'conj.',
            'SCONJ': 'conj.'
        }
        statistical_res = LexiAnnot.analyze_ass_language(ass_file)
        main_style = LexiAnnot.select_main_style_weighted(statistical_res, lang)
        if not main_style:
            logger.error(f'æ— æ³•ç¡®å®šä¸»è¦å­—å¹•æ ·å¼')
            return None
        index = 0
        lines_to_process = []
        main_dialogue: Dict[int, SSAEvent] = {}
        for dialogue in ass_file:
            if dialogue.style != main_style:
                continue
            time_code = (dialogue.start, dialogue.end)
            text_raw = dialogue.plaintext
            line_data = {'index': index, 'time_code': time_code, 'raw_subtitle': text_raw, 'new_vocab': [],
                         'Chinese': ''}
            lines_to_process.append(line_data)
            main_dialogue[index] = dialogue
            index += 1
        lines_to_process = self.__process_by_ai(lines_to_process, cefr_lexicon, swear_words, coca20k_lexicon)

        # åœ¨åŸå­—å¹•æ·»åŠ æ ‡æ³¨
        main_style_fs = ass_file.styles[main_style].fontsize
        for line_data in lines_to_process:
            if self._shutdown_event.is_set():
                return None
            if line_data['new_vocab']:
                replacements = line_data['new_vocab']
                for replacement in replacements:
                    part_of_speech = f"{{\\fnTimes New Roman\\fs{int(main_style_fs * 0.75)}\\i1}}{pos_map[replacement['pos']]}{{\\r}}"
                    new_text = f"{{\\c{abgr_str}}}{replacement['text']}{{\\r}}"
                    if self._in_place:
                        new_text = new_text + f" ({replacement['Chinese']} {part_of_speech})" if replacement[
                            'Chinese'] else ""
                    else:
                        dialogue = pysubs2.SSAEvent()
                        dialogue.start = main_dialogue[line_data['index']].start
                        dialogue.end = main_dialogue[line_data['index']].end
                        dialogue.style = 'Annotation EN'
                        cefr_text = f" {{\\rAnnotation CEFR}}{replacement['cefr']}{{\\r}}" if replacement[
                            'cefr'] else ""
                        __N = r'\N'
                        phone_text = f"{__N}{{\\rAnnotation PHONE}}/{replacement['phonetics']}/{{\\r}}" if replacement[
                                                                                                               'phonetics'] and self._show_phonetics else ""
                        annot_text = f"{replacement['lemma']} {{\\rAnnotation POS}}{pos_map[replacement['pos']]}{{\\r}} {{\\rAnnotation ZH}}{replacement['Chinese']}{{\\r}}{cefr_text}{phone_text}"
                        dialogue.text = annot_text
                        ass_file.append(dialogue)
                        if self._show_vocabulary_detail and replacement['pos_defs']:
                            dialogue = pysubs2.SSAEvent()
                            dialogue.start = main_dialogue[line_data['index']].start
                            dialogue.end = main_dialogue[line_data['index']].end
                            dialogue.style = 'DETAIL CN'
                            detail_text = []
                            for pos_def in replacement['pos_defs']:
                                meaning_str = ', '.join(pos_def['meanings'])
                                pos_text = f"{{\\rDETAIL POS}}{pos_def['pos']}{{\\r}} {meaning_str}"
                                detail_text.append(pos_text)
                            dialogue.text = '\\N'.join(detail_text)
                            ass_file.append(dialogue)
                    replacement['new_text'] = new_text
                LexiAnnot.replace_by_plaintext_positions(main_dialogue[line_data['index']], replacements)
            if self._sentence_translation:
                chinese = line_data['Chinese']
                if chinese and chinese[-1] in ['ã€‚', 'ï¼Œ']:
                    chinese = chinese[:-1]
                main_dialogue[line_data['index']].text = main_dialogue[line_data['index']].text + f"\\N{chinese}"
        return ass_file

